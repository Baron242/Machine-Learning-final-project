{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "784b0dcab8bb422cbdcc0fdfb763e341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "30124\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torchtext.data\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Load CSV files\n",
    "train_df = pd.read_csv('imdb_train.csv')\n",
    "test_df = pd.read_csv('imdb_test.csv')\n",
    "\n",
    "# Define a tokenizer\n",
    "tokenizer = torchtext.data.get_tokenizer('basic_english')\n",
    "\n",
    "# Function to iterate over tokens\n",
    "def iterate_tokens(df):\n",
    "    for review in tqdm(df['review']):\n",
    "        yield tokenizer(review)\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = build_vocab_from_iterator(\n",
    "    iterate_tokens(train_df),\n",
    "    min_freq=5,\n",
    "    specials=['<unk>', '<s>', '<eos>']\n",
    ")\n",
    "\n",
    "vocab.set_default_index(0)\n",
    "\n",
    "# Print the length of the vocabulary\n",
    "print(len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0244996d0374359b32de3db4539e508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Sum of input lengths does not equal the length of the input dataset!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-419ec7dd5c25>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;31m# Split dataset into training and validation sets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m \u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;31m# Create DataLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py36\\lib\\site-packages\\torch\\utils\\data\\dataset.py\u001b[0m in \u001b[0;36mrandom_split\u001b[1;34m(dataset, lengths, generator)\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;31m# Cannot verify that dataset is Sized\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Sum of input lengths does not equal the length of the input dataset!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m     \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandperm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Sum of input lengths does not equal the length of the input dataset!"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import (\n",
    "    TensorDataset,\n",
    "    DataLoader,\n",
    "    random_split,\n",
    ")\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pandas as pd\n",
    "import torchtext.data\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Load CSV files\n",
    "train_df = pd.read_csv('imdb_train.csv')\n",
    "test_df = pd.read_csv('imdb_test.csv')\n",
    "\n",
    "# Define a tokenizer\n",
    "tokenizer = torchtext.data.get_tokenizer('basic_english')\n",
    "\n",
    "# Function to iterate over tokens\n",
    "def iterate_tokens(df):\n",
    "    for review in tqdm(df['review']):\n",
    "        yield tokenizer(review)\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = build_vocab_from_iterator(\n",
    "    iterate_tokens(train_df),\n",
    "    min_freq=5,\n",
    "    specials=['<unk>', '<s>', '<eos>']\n",
    ")\n",
    "\n",
    "vocab.set_default_index(0)\n",
    "\n",
    "# Convert text sequences to indices using the vocabulary\n",
    "sequences = [\n",
    "    torch.tensor(vocab.lookup_indices(tokenizer(review)), dtype=torch.int64)\n",
    "    for review in train_df['review']\n",
    "]\n",
    "\n",
    "# Pad sequences\n",
    "padded_sequences = pad_sequence(sequences, batch_first=True)[:, :250]\n",
    "sentiments = torch.tensor(train_df['sentiment'], dtype=torch.int64)\n",
    "\n",
    "# Create TensorDataset\n",
    "dataset = TensorDataset(padded_sequences, sentiments)\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "(train_dataset, val_dataset) = random_split(dataset, (0.7, 0.3))\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "val_dataloader = DataLoader(val_dataset, shuffle=True, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98c262d90d864f6290c8752f05a55437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pandas as pd\n",
    "import torchtext.data\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Load CSV files\n",
    "train_df = pd.read_csv('imdb_train.csv')\n",
    "test_df = pd.read_csv('imdb_test.csv')\n",
    "\n",
    "# Define a tokenizer\n",
    "tokenizer = torchtext.data.get_tokenizer('basic_english')\n",
    "\n",
    "# Function to iterate over tokens\n",
    "def iterate_tokens(df):\n",
    "    for review in tqdm(df['review']):\n",
    "        yield tokenizer(review)\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = build_vocab_from_iterator(\n",
    "    iterate_tokens(train_df),\n",
    "    min_freq=5,\n",
    "    specials=['<unk>', '<s>', '<eos>']\n",
    ")\n",
    "\n",
    "vocab.set_default_index(0)\n",
    "\n",
    "# Convert text sequences to indices using the vocabulary\n",
    "sequences = [\n",
    "    torch.tensor(vocab.lookup_indices(tokenizer(review)), dtype=torch.int64)\n",
    "    for review in train_df['review']\n",
    "]\n",
    "\n",
    "# Pad sequences\n",
    "padded_sequences = pad_sequence(sequences, batch_first=True)[:, :250]\n",
    "sentiments = torch.tensor(train_df['sentiment'], dtype=torch.int64)\n",
    "\n",
    "# Create TensorDataset\n",
    "dataset = TensorDataset(padded_sequences, sentiments)\n",
    "\n",
    "# Calculate lengths for training and validation sets\n",
    "total_length = len(dataset)\n",
    "train_length = int(0.7 * total_length)\n",
    "val_length = total_length - train_length\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "(train_dataset, val_dataset) = random_split(dataset, (train_length, val_length))\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "val_dataloader = DataLoader(val_dataset, shuffle=True, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch_lightning in c:\\users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages (1.5.10)\n",
      "Requirement already satisfied: numpy>=1.17.2 in c:\\users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages (from pytorch_lightning) (1.19.5)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in c:\\users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages (from pytorch_lightning) (2.10.1)\n",
      "Requirement already satisfied: torchmetrics>=0.4.1 in c:\\users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages (from pytorch_lightning) (0.8.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages (from pytorch_lightning) (3.7.4.2)\n",
      "Requirement already satisfied: PyYAML>=5.1 in c:\\users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages (from pytorch_lightning) (5.3.1)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages (from pytorch_lightning) (20.4)\n",
      "Requirement already satisfied: future>=0.17.1 in c:\\users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages (from pytorch_lightning) (0.18.2)\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in c:\\users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages (from pytorch_lightning) (2022.1.0)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in c:\\users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages (from pytorch_lightning) (4.47.0)\n",
      "Requirement already satisfied: pyDeprecate==0.3.1 in c:\\users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages (from pytorch_lightning) (0.3.1)\n",
      "Requirement already satisfied: torch>=1.7.* in c:\\users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages (from pytorch_lightning) (1.10.2)\n",
      "Requirement already satisfied: setuptools==59.5.0 in c:\\users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages (from pytorch_lightning) (59.5.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.19.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.3.7)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.34.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in c:\\users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.48.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning) (2.24.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning) (2.22.0)\n",
      "Requirement already satisfied: six in c:\\users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages (from packaging>=17.0->pytorch_lightning) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages (from packaging>=17.0->pytorch_lightning) (2.4.7)\n",
      "Requirement already satisfied: aiohttp; extra == \"http\" in c:\\users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.8.6)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in c:\\users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages (from torch>=1.7.*->pytorch_lightning) (0.8)\n",
      "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in c:\\users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (4.8.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning) (1.25.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (1.3.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.3.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages (from aiohttp; extra == \"http\"->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (19.3.0)\n",
      "Requirement already satisfied: asynctest==0.13.0; python_version < \"3.8\" in c:\\users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages (from aiohttp; extra == \"http\"->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (0.13.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages (from aiohttp; extra == \"http\"->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages (from aiohttp; extra == \"http\"->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.2.0)\n",
      "Requirement already satisfied: idna-ssl>=1.0; python_version < \"3.7\" in c:\\users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages (from aiohttp; extra == \"http\"->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages (from aiohttp; extra == \"http\"->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages (from aiohttp; extra == \"http\"->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (5.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages (from aiohttp; extra == \"http\"->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages (from aiohttp; extra == \"http\"->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.7.2)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (3.1.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.2.2)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.5.1)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "!pip install pytorch_lightning\n",
    "\n",
    "from pytorch_lightning import LightningModule\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "class MySequenceClassifier(LightningModule):\n",
    "    def __init__(self, vocab_size, dim_emb, dim_state):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, dim_emb)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=dim_emb,\n",
    "                          hidden_size=dim_state,\n",
    "                          num_layers=1,\n",
    "                          batch_first=True)\n",
    "        \n",
    "        self.output = nn.Linear(dim_state, 2)\n",
    "        \n",
    "        # will be monitoring accuracy\n",
    "        self.accuracy = Accuracy(task='multiclass', num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim\n",
    "#\n",
    "# the rest\n",
    "#\n",
    "class MySequenceClassifier(MySequenceClassifier):\n",
    "    def forward(self, seq_batch):\n",
    "        emb = self.embedding(seq_batch)\n",
    "        _, (state, _) = self.lstm(emb)\n",
    "        # state: (num_layers, batch, dim_state)\n",
    "        output = self.output(state[-1])\n",
    "        return output\n",
    "    def loss(self, outputs, targets):\n",
    "        return nn.functional.cross_entropy(outputs, targets)\n",
    "    \n",
    "    def training_step(self, batch, batch_index):\n",
    "        inputs, targets = batch\n",
    "        outputs = self.forward(inputs)\n",
    "        loss = self.loss(outputs, targets)\n",
    "        self.accuracy(outputs, targets)\n",
    "        self.log('acc', self.accuracy, prog_bar=True)\n",
    "        self.log('loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters())\n",
    "    \n",
    "    def validation_step(self, batch, batch_index):\n",
    "        inputs, targets = batch\n",
    "        outputs = self.forward(inputs)\n",
    "        self.accuracy(outputs, targets)\n",
    "        self.log('val_acc', self.accuracy, prog_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "logger = CSVLogger('./lightning_logs/', 'lstm')\n",
    "\n",
    "trainer = Trainer(max_epochs=10, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type      | Params\n",
      "----------------------------------------\n",
      "0 | embedding | Embedding | 963 K \n",
      "1 | lstm      | LSTM      | 25.1 K\n",
      "2 | output    | Linear    | 130   \n",
      "3 | accuracy  | Accuracy  | 0     \n",
      "----------------------------------------\n",
      "989 K     Trainable params\n",
      "0         Non-trainable params\n",
      "989 K     Total params\n",
      "3.957     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:662: UserWarning: Your `val_dataloader` has `shuffle=True`, it is strongly recommended that you turn this off for val/test/predict dataloaders.\n",
      "  category=UserWarning,\n",
      "C:\\Users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "C:\\Users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b35f3a95915748199ae31f4fd73d35d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baron\\anaconda3\\envs\\py36\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "model = MySequenceClassifier(vocab_size=len(vocab),\n",
    "                             dim_emb=32,\n",
    "                             dim_state=64)\n",
    "\n",
    "trainer.fit(model,\n",
    "            train_dataloaders=train_dataloader,\n",
    "            val_dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
